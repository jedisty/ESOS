% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis &lt;douglis@research.att.com&gt; 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

% This version uses the latex2e styles, not the very ancient 2.09 stuff.
%\documentclass[letterpaper,twocolumn,10pt]{article}
%\usepackage{usenix,epsfig,endnotes}

%\documentclass[preprint,nocopyrightspace]{sigplanconf-eurosys}
%\documentclass[letterpaper,twocolumn,10pt]{article}

\documentclass[10pt,twocolumn,conference]{IEEEtran}

\usepackage{epsfig,endnotes}
\usepackage{kotex}
\usepackage{subfigure}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{color}
\newenvironment{translatedtext}[2]
   {{\bfseries \color{blue} #1} 
    {\bfseries \color{red}  #2}}

\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

%don't want date printed
%\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{Unified Storage Layer: Orchestrating the
Log-Structured Filesystem and the Flash Storage} 

\author[1]{Jinsoo Yoo}
\author[1]{Joontaek Oh}
\author[1]{Seongjin Lee}
\author[1]{Youjip Won}
\author[2]{Jin-Yong Ha}
\author[2]{Jongsung Lee}
\author[2]{Junseok Shim}
\affil[1]{Hanyang University}
\affil[ ]{\{jedisty $|$ na94jun $|$ insight $|$ yjwon\} @hanyang.ac.kr}
\affil[2]{Samsung Electronics}
\affil[ ]{\{jy200.ha $|$ js0007.lee $|$ junseok.shim\} @samsung.com}

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
%\thispagestyle{empty}

\begin{abstract}
In this work, we develop a new IO stack, \emph{Unified Storage Layer,
  USL}, which vertically integrates the log structured filesystem and
the Flash based storage device. When the log-structured filesystem are 
used for flash storage, the I/O stack becomes vulnerable to \emph{Compound
  Garbage Collection} problem; the notion describes a redundant effort
to garbage collect some data which are already cleaned on
filesystem. USL consists of three key technical ingredients: (i)
Direct binding of filesystem sections and storage superblocks, which
are garbage collection units, (ii) Filesystem I/O patching to align
the I/O size with physical Flash page size, and (iii) Disaggregate
mapping to de-virtualize subset of Flash blocks statically bind them
with filesystem segments. The design of USL makes two things possible
which bears profound implications in I/O stack design. First, USL
dispenses with the need of large address translation table. Second,
and more importantly, static binding entirely eliminates the root
cause for compound garbage collection. We implement USL using F2FS and
Samsung 843Tn SSD. Our result shows that USL effectively reduces the
size of SSD metadata by 1/54. It also reduces the write amplification
by 40$\%$ and increases random write IOPS by 77$\%$.
\end{abstract}

\begin{comment}
In this work, we develop a new IO stack, \emph{Unified Storage Layer,
  USL}, which vertically integrates the log structured filesystem and
the Flash based storage device. The filesystem and Flash based storage
have evolved in their own way to optimize itself against the other.
The Flash storage adopts sophisticated software layer called Flash
translation layer to hide its append-only nature from the in-place
update based filesystem. The log-structured filesystem has been
proposed to relieve a Flash storage from expensive address translation
and the garbage collection overhead via maintaining the filesystem
blocks in append-only manner. Despite their elegance, when they are
combined, the IO stack becomes subject to unacceptable performance
deficiency primarily due to the redundant effort to make the room to
accommodating the newly arriving data in both layers. We call this
phenomenon as \emph{Compound Garbage Collection}.  The Unified Storage
Layer consists of three key technical ingredients: (i) superblock to
segment static binding where the individual superblocks in SSD are
statically bound to the individual system segments, (ii) Block
Patching in the filesystem to align the IO size with the Flash page
size, and (iii) Disaggregate Mapping which allows the Flash storage to
maintain a subset of its Flash blocks by itself, not statically bound
to the filesystem segment.  This static binding bears profound
implications in IO stack design. First, the SSD can dispense with
address translation layer since the filesystem in USL maintains the
mapping between the filesystem blocks to the physical Flash
pages. Second, more importantly, the static binding entirely
eliminates the root cause for compound garbage collection since the
segment cleaning activity of the log-structured filesystem directly
consolidates the valid Flash pages in the Flash storage.

We implement a prototype Unified Storage Layer. We use F2FS (Flash
Friendly Filesystem) and Samsung 843Tn SSD as the baseline platform and
modify each of them to form a Unified Storage layer. USL reduces the SSD
mapping table size by 1/54. USL effectively eliminates the root cause
for compound garbage collection. Subsequently, compared to F2FS over
commodity Flash storage, the USL reduces the write amplification by
40$\%$ and increases random write performance by 77$\%$.
\end{comment}


\section{Introduction}

The advent of NAND Flash memory and many of its favorable
characteristics such as low I/O latency, low power, and shock
resistance led to wide spread of Solid State Drives (SSDs). Few other
driving forces behind its popularity can be reduced to the price and
capacity. The price of 1 Gbyte of NAND devices are now well under a
dollar \cite{ssdprice}, and 3D-stacking technology \cite{3dxpoint} has
opened a door to increase the capacity of SSDs substantially. However,
as the technology moves towards storing more bits in NAND Flash cells,
the reliability and metadata management overhead of the device are the
two issues that require constant attention.

One crucial complications of SSDs which makes it so much different
from HDDs is that it has to be erased before any data can be in-place
updated. But, the unit and latency of read/program and erase operation
is not the same. The unit of read/program is a page, which has size of
4 to 16 Kbyte, and unit of erase is a block, which is group of 128 to
256 pages. And, latency of erase operation (e.g. 1.5 msec
\cite{samsung_Flash}) is about two times slower than that of write
operation (e.g. 0.8 msec \cite{samsung_Flash}). Thus, the state-of-the-art 
firmware, called Flash Translation Layer
(FTL), not only has to hide asymmetries between read/program and erase
operations, but also has to translate in-place update requests from
filesystem to out-of-place writes in storage device. The most
important job of FTL is not only to provide such abstraction but also
to hide the overhead of garbage collecting dirty pages which is
generated as a result of performing out-of-place updates. 

\begin{comment}
  Because of
  this, SSDs are considered as one form of log system and
  share similar characteristics with log-structured filesystems
  \cite{rosenblum1992design, lee2015f2fs, woodhouse2001jffs,
  engel2005logfs}.
\end{comment}

\begin{comment}
  For example, when a SSD receives a
  request to update LBA $n$, which was originally in page $i$ of block
  $m$ in SSD, FTL invalidates page $i$ and searches for available free
  page in block $m$ or in other blocks. Then, FTL writes updated data to
  the found free page. Actual physical location of LBA $n$ is subject
  to change, but its job of FTL to keep the account of all changes in
  mapping table.
\end{comment}

Researchers in the field have proposed to use log-structured
filesystems \cite{lee2015f2fs, nilfs2006} or application
\cite{lim2011silt} to reduce the garbage collection overhead and hide
the asymmetricity between read/program and erase operations. They make
use of the fact that log-structured system places all write requests
in out-of-place manner and show sequential write pattern which are
handled efficiently in SSDs. Although using log-structured system can
increase the bandwidth, however, stacking log on top of other log
system creates new sets of problems \cite{yang2014don}. 

The first issue is the overhead of
keeping the mapping table on both layers. It is a known fact that most 
of commercial SSDs makes use of page mapping for performance reasons 
\cite{ban1995Flash}. But, it comes with the cost of large memory 
requirement to cache the entire mapping table, e.g., the size of page 
mapping table in 4 Tbyte SSD with 4 Kbyte page is 4 Gbyte.
The large memory footprint of page mapping requires SSD to adopt larger 
DRAM memory. Fig \ref{fig:dram_size} shows that the size of DRAM increased 
considerably over the years. It shows that about 60$\%$ of SSDs have 512 Mbyte and 1 Gbyte of DRAM.
\begin{translatedtext}{
Several sophisticated mapping schemes \cite{kim2002space, fast07, last08, kang2006superblock}
proposed to reduce the mapping table size, but the downside is that 
such SSDs cannot handle random write workload efficiently \cite{kim2002space}.
SSD 뿐만 아니라 호스트에서 동작하는 로그 시스템들 또한 이러한 매핑 테이블을
 메모리에 관리해야 하므로, address indirection을 위한 메타데이터 관리
overhead를 증가시킨다.
}{
There are several sophisticated mapping schemes \cite{kim2002space,
  fast07, last08, kang2006superblock} that reduces the mapping table
size, but they fail to handle random write workloads efficiently
\cite{kim2002space}. As a result, the overhead of keeping the metadata
is still an issue because both of the log system in SSD and in the
host has to keep their mapping table as well as metadata to make
address indirections in the main memory. 
}\end{translatedtext}

\begin{figure}[h]
\begin{center}
\includegraphics[width=3.2in]{./figure/dram_size.eps}
\caption{RW: DRAM size Trend in SSDs (From 2011 to 2015)}
\label{fig:dram_size}
\end{center}
\end{figure}

Stacking log system generate more
severe issue called compound garbage collection---the phenomenon where
log structured storage performs garbage collection on data that
log-structured filesystem already completed performing segment
cleaning. As the result of compound garbage collection, storage has to
re-write data that is already in the storage device and recreate
segment cleaning overhead of filesystem on storage. 
Thus, compound garbage collection amplifies the volume of I/Os which
not only degrades overall system performance but also has severe
effect on life span of SSD device.

\begin{comment}
  Since the advent of Flash memory, people have anticipated the need of
  mapping table and proposed several sophisticated mapping schemes
  \cite{kim2002space, fast07, last08, kang2006superblock, dftl09};
  however, it is a known fact that most of commercial SSDs makes use of
  page mapping for performance reasons \cite{ban1995Flash}. But, it
  comes with the cost of large memory requirement to cache the entire
  mapping table, e.g., the size of page mapping table in 4 Tbyte SSD
  with 4 Kbyte page is 4 Gbyte. The size may be reduced by using block
  mapping, but the downside is that such SSDs cannot handle random write
  workload efficiently \cite{kim2002space}. If we are to have both low
  memory footprint and high performance, then all random writes have to
  be translated to sequential writes. Considering all, log-structured
  filesystem seems to be the best fit for the problem.
\end{comment}

In this paper, we introduce Unified Storage Layer (USL) to address the two
issues in exploiting the stacking log-structured system, which are the
size of mapping table and compound garbage collection.
By integrating address indirection layers on host and SSD into
filesystem, USL successfully addresses size of the mapping table and compound
garbage collection issue. 
USL is collection of modified log-structured filesystem (F2FS) and a SSD with
disaggregate mapping scheme. In essence, disaggregate mapping scheme
allows reducing the mapping table size to 1/54 of page mapping. In
this scheme, each logical address of a section, which is the segment
cleaning unit in F2FS, is binded with physical address of a block,
which is the garbage collection unit of SSD. USL resolves compound
garbage collection issue by delegating the job of device garbage
collection to filesystem. We modify the firmware of 843Tn SSD
\cite{ssd843tn} and F2FS to implement USL and compare the performance
with existing systems. The write amplification in USL is about 40$\%$
lower and IOPS is 77$\%$ higher compared to that of base F2FS in
stacking log-structured system.

\section{Background}

\subsection{Segment Cleaning in Log-structured Filesystem}

Log-structured filesystem buffers all updates in an in-memory structure
which is called segment. When the segment is full, the filesystem
writes the updated data to disk in append-only manner. 
Since all writes in a log structured filesystem are performed in units
of segment, the system can minimize disk seek times and maximize the
write perform by sequentially logging the segments.
When the log-structured filesystem receives updates to existing data,
it invalidates old data and appends the new data at the end of the
segment. The invalidated data has to be cleaned at some point in time,
and the process is called segment cleaning. All log-structured system
spend considerable amount of time in reclaiming the space used
by invalidated data blocks and copying valid blocks in such
segments. Thus, the excessive segment cleaning implementation can 
degrade the performance of log-structured filesystem.

In recent years, the log-structured filesystem has
been reevaluated because of its out-of-update characteristic
which is well aligned with the behavior of SSD. For example, 
Flash-Friendly Filesystem (F2FS) is designed to fit in flash storage.
Fig. \ref{fig:f2fs_partition} illustrates the layout of F2FS
partition. There are two areas in F2FS filesystem
partition: Metadata area, which keeps filesystem related metadata and
Main area, which keeps user generated data along with file related
metadata (node). F2FS는 Metadata area에는 in-place update 방식으로
데이터를 기록하며,
Main area는 파일시스템 partition의 utilization에 따라서 append-only logging 
(낮은 Utilization 일 때)과 in-place update (높은 Utilization 일 때)를 
선택적으로 수행한다. Main area에는 최대 6개의 append point를 
사용하며, 이는 3개의 hotness (Hot, Warm, Cold)와 2개의 Type (Data, Node)의 조합이다.
F2FS는 이러한 data clustering을 통해, 비슷한 life time을 갖는 데이터들을
동일한 segment에 모아둠으로써, segment cleaning으로 인한 overhead를
감소시킨다.
The unit of garbage collection in F2FS
is a section which is defined as groups of segments---by default it
is set as one segment in a section. A victim section in a segment
cleaning process is decided by two segment cleaning policies called
foreground and background segment cleaning, which are base on greedy
\cite{kawaguchi1995Flash} and cost benefit
\cite{rosenblum1992design}, respectively.

\begin{comment}
  A log-structured filesystem is a append only system. All writes are
  sequential which makes it ideal for SSDs. In log-structured
  filesystem, an entire filesystem is divided with a unit called
  segment, which is a set of consecutive filesystem blocks. Upon
  receiving a write request, the log filesystem allocates a free segment
  for data and use it to append more data until the segment is
  full. Note that the size of segment varies, e.g., 8 Mbyte in NILFS2
  \cite{nilfs_segment_size}, 512Kbyte to 1Mbyte in Sprite-LFS
  \cite{rosenblum1992design}, and 2 Mbyte in F2FS
  \cite{f2fs_segment_size}. Note that F2FS is a fairly young log
  filesystem which targets to assimilate the characteristics of Flash
  memory. Recent report on F2FS \cite{lee2015f2fs} shows that its
  performance is about 3.1 times better than EXT4 \cite{cao2007ext4} on
  some workloads.
  % Jeong et al. \cite{jeong2013stack} showed that F2FS is also well suited for mobile workloads. 
  Fig. \ref{fig:f2fs_partition} illustrates the layout of F2FS
  filesystem partition. There are two areas in F2FS filesystem
  partition: Metadata area, which keeps filesystem related metadata and
  Main, which area keeps user generated data along with file related
  metadata (node).
\end{comment}


\begin{figure}[t]
\begin{center}
\includegraphics[width=3.2in]{./figure/f2fs_layout}
\caption{F2FS Partition Layout}
\label{fig:f2fs_partition}
\vspace{-1.9em}
\end{center}
\end{figure}


\begin{comment}
  F2FS triggers segment cleaning when the number of free segments goes
  under predefined threshold. The unit of garbage collection in F2FS
  is a section which is defined as groups of segments---by default it
  is set as one segment in a section. A victim section in a segment
  cleaning process is decided by two segment cleaning policies called
  foreground and background segment cleaning, which are base on greedy
  \cite{kawaguchi1995Flash} and cost benefit
  \cite{rosenblum1992design}, respectively. Once the victim is
  selected, valid data in the sections are copied to the current
  segment and reverts the section to free section. Note that segment
  cleaning process impedes user I/O operations because it is
  non-preemptive operation.
\end{comment}

\subsection{Garbage Collection in Flash Storage}

Since SSDs exhibit log-structured like behaviors, garbage collection
of invalid pages is a must for the storage. 
% It is because FTL performs all updates in out-of-place manner,
% therefore the storage needs to handle the invalid data. 
Similar to segment cleaning in filesystem, SSD also runs garbage
collection when there is not enough empty space in the storage. The
unit of garbage collection in SSD is a NAND block which is analogous to a
segment in the filesystem segment cleaning. Once garbage collection
algorithm selects a victim block, valid pages in the block are copied
to an empty pages in other blocks. Then, it erases the block to revert
it as an empty block.


\begin{figure}[t]
\begin{center}
\includegraphics[width=3.2in]{./figure/ssd_internal.eps}
\caption{Block Diagram of an SSD}
\label{fig:ssd_internal}
\end{center}
\vspace{-1.9em}
\end{figure}


Flash memories cannot serve any read/program operations
during a garbage collection process. One way to hide the latency of
processing garbage collection is to exploit the parallelism using
multi-channel configuration. Typical SSDs group NAND Flash memories in
channels and ways to exploit the parallelism and to maximize the I/O
performance.  Fig. \ref{fig:ssd_internal} illustrates the architecture
of an SSD with 2 channel/2 way configuration. When a channel of Flash
memories are busy with handling garbage collection process, the other
channel can be used to handle the I/O requests.

\begin{comment}
  Kang et al. \cite{kang2006superblock} introduced Superblock FTL for
  In multi-channel/multi-way configuration; ingenuity of Superblock
  FTL is the introduction of the notion called super-block, which is a
  group of pages with same offset in NAND blocks on each channel and
  ways. When Superblock FTL receives a write request, a empty
  super-block is allocated to store the data. This process repeats
  until there is no free pages in the current super-block. It
  naturally makes most out of multi-channel and multi-way parallelism
  of the SSD. Since the garbage collection in Superblock FTL also
  exploits super-block as a unit, SSD suffers from garbage collection
  overhead dearly because it cannot handle any of read/program
  requests.
\end{comment}

\begin{comment}

\subsection{Log-structured Filesystem and SSD}

  Originally, log-structured filesystem came out to improve the random
  write performance of slow HDDs. Even though the idea of exploiting
  sequential performance of the device captured many researchers
  attention, but the fact that there is the overhead of garbage
  collection made many hesitant in adopt it in a running system.

  Behind the scene, FTL manages mapping between logical to physical addresses and takes care
  of out-of-place updates. Researchers in the field have proposed many
  different mapping schemes such as page level FTL \cite{ban1995Flash},
  Block FTL \cite{kim2002space}, and hybrid FTLs
  \cite{kang2006superblock, fast07, last08}.

  Page level FTL has been the de facto mapping
  scheme used in the industries. however, large memory footprint
  requires SSD to adopt larger DRAM memory. Fig \ref{fig:dram_size}
  shows that the size of DRAM increased considerably over the years. It
  shows that about 60$\%$ of SSDs have 512 Mbyte and 1 Gbyte of DRAM.
\end{comment}


\begin{comment}
  A reasons behind the growth of DRAM size is to reflect the increased
  size of SSD metadata such as mapping table. Instead, it can exploit block level mapping which
  is better for sequential workloads and also can significantly reduce the
  size of mapping information. A 256 Gbyte SSD
  with 4 Kbyte page and 2 Mbyte of block, the page level FTL needs 
  256 Mbyte of mapping table, but block FTL only needs to
  store 512 Kbyte of information. Since log filesystem is aimed for  
  generating sequential workloads, it would be ideal to match log
  filesystems with SSDs, then it can exploit block level mapping.
\end{comment}

\vspace{-0.5em}
\subsection{Stacking the Logs}
There is Flash-aware filesystem \cite{lee2015f2fs} and application
\cite{lim2011silt} that exploits Flash memory based storage in a
system to make best use of out-of-place update characteristics of
Flash memory device and to improve the storage performance. Both
approaches introduces a new log structured layer to a system already
with a log-structured layer, i.e., SSD. When two or more log systems
are working in layers, we call them stacked log system. Fig
\ref{fig:layered_log_system} illustrates an example of a stacked log
system with a log structured filesystem over an SSD.

As Yang et al. \cite{yang2014don} analyzed, the problem in stacked log
system is that as more log structured layers stacked together the more
the overhead of managing metadata and the garbage collection. Each
layer has to manage its own metadata to keep information for address
mapping and as a result larger main memory is required to load the
metadata. Moreover, each layer performs garbage collection of its
managing address space. When the upper log structured layer performs
garbage collection while lower log system is also performing garbage
collection can cause severe performance issue which we define it as
compound garbage collection. We discuss the issue in more detail in
Section \ref{sec:CompoundGC}.

\begin{comment}
  Two log-structured systems stacked on top of each other is called as
  stacking log system, and an example of such system is illustrated in
  Fig \ref{fig:layered_log_system}. It shows that log filesystem is on
  top of Flash based storage device. When virtual file system sends data
  down to filesystem, the data is recorded in the filesystem
  partition. When there is not enough space left in the partition, the
  filesystem runs segment cleaning to tidy up the invalid data. As a
  result, the filesystem has to send not only the data received from the
  virtual file system but also the writes generated by the segment
  cleaning.

  When the device writes the received updates on the storage medium, 
  FTL decides where to put the data. When there is not enough space in
  the storage, SSD runs garbage collection to reclaim free pages for
  incoming data. Initial write volume increases as it is passed down in
  the I/O hierarchy, which is typical in log-structured systems. And, it
  is called Write Amplification.  Larger write amplification means
  trouble for SSD because it not only has to handle more data but also
  reduces the life of the device.
\end{comment}


\begin{figure}[t]
\begin{center}
\includegraphics[width=3in]{./figure/layered_log_system}
\caption{Example of Stacking Log System}
\label{fig:layered_log_system}
\end{center}
\end{figure}

\begin{comment}
  Since erase operation in SSDs
  have larger latency (e.g. 1.5 msec \cite{samsung_Flash}) than program
  operation (e.g. 800 $\mu$sec \cite{samsung_Flash}), all writes are not
  in-place updated but out-of-place updated just like the log structured
  filesystem. Note that the existing data is invalidated when the new
  data is stored in else where. This out-of-place update behavior calls
  for garbage collection. Thus, the final data written on the storage
  device includes both data received from the filesystem and writes
  generated from garbage collecting the device.
\end{comment}

\begin{comment}
  There are redundancies in the stacking log system; both layers have to
  keep mapping table to manage the whereabouts of appended data. Suppose
  a log filesystem decides the physical location of the data in SSD,
  then practically the SSD does not have to maintain a mapping table to
  keep account of the updates. Note that mapping table overhead is
  greater in SSDs, not only because the size of DRAM in SSDs are much
  smaller than the host system but also unit and latency of operations
  are different.
\end{comment}


\begin{comment}
  \subsection{Case Study 1: Uncoordinated Garbage Collection}
  \label{subsec:case_study_1}

  In a log-structured system, update of an existing data is processed in
  two steps: first, the updated data is stored in a free space and
  second, the existing data is invalidated. However, when log-structured
  system is layered on top of each other, update of an existing data can
  be processed differently. Fig. \ref{fig:comp_gc_2} illustrates the
  case of uncoordinated garbage collection. In this example, let us
  assume a block in a SSD has ten pages; for simplicity of the example,
  the size of the page is 512 byte, and segment 0 holds LBA 0 to LBA 3
  and segment 1 holds LBA 4 to LBA 7.  Fig. \ref{fig:comp_gc_2}(a) shows
  the state of a filesystem with file $A$ which has length of four
  logical blocks; data blocks from $A_1$ to $A_4$ are stored in LBA 0 to
  LBA 3, respectively. Upon synchronization of data in segment 0, the
  storage device stores LBA 0 through LBA 3 on page from 0 to 3 in block
  0. Fig. \ref{fig:comp_gc_2}(b) illustrates the state of the system
  after $A_1$ through $A_3$ are updated in segment 0. Updated data from
  $A_1$ to $A_3$ is appended to free segment, which in this case is
  segment 1; updated version of $A_1$ to $A_3$ are stored in LBA 4 to
  LBA 6, and they are physically placed in page 4 to page 6,
  respectively. Note that storage device has no idea whether the data
  is updated version of existing data; thus, the storage device does not
  invalidate the pages from 0 to 3, on the contrary, LBAs from 0 to 3 in
  the filesystem are invalidated.

  Suppose segment cleaning and garbage collection on both layers are
  triggered simultaneously, the filesystem may clean the invalid file
  blocks $A_1$ to $A_3$, but since the pages in the SSD are not
  invalidated, the old data is left in the storage. In some scenarios,
  garbage collection of block 0 may even copy the pages from 0 to 2. 

  As shown in the example, uncoordinated garbage collection describes
  the case where blocks in filesystem are invalidated, but storage
  device is unaware of the fact and performs garbage collection to copy
  the corresponding data. Fortunately, TRIM command \cite{shu2007data}
  solves uncoordinated garbage collection, which is a SATA command to
  sends the list of invalidated filesystem LBAs to SSDs. Upon receiving
  the command, SSD also invalidates the list of LBAs in pages and does
  not copy the pages invalidated via the command. It is important to
  note that TRIM command is not the absolute solution for uncoordinated
  garbage collection because filesystem does not send TRIM command
  immediately when invalid LBAs are generated. F2FS, for example, sends
  TRIM command when it checkpoints the filesystem. Section
  \ref{subsec:io_performance} provides experiment results on
  uncoordinated garbage collection.

  \begin{figure}[t]
  \begin{center}
  \includegraphics[width=3in]{./figure/comp_gc_scenario_2}
  \caption{Example of Uncoordinated Garbage Collections}
  \label{fig:comp_gc_2}
  \vspace{-1.5em}
  \end{center}
  \end{figure}

  \subsection{Case Study 2: Compound Garbage Collection}
  \label{subsec:case_study_2}
\end{comment}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=5.7in]{./figure/comp_gc_scenario_1}
\caption{Example of Compound Garbage Collection ($Ln$ means LBA $n$)}
\label{fig:comp_gc_1}
\end{center}
\vspace{-1.5em}
\end{figure*}

\section{Garbage Collection Problem}
\label{sec:CompoundGC}

In this section we illustrate compound garbage collection issue 
in stacking log systems.
We define compound garbage collection as the case where the storage
level log system performs garbage collection on data blocks which are
already segment cleaned by filesystem.
Because compound garbage collection leads to high write amplification, 
it can degrade the performance of the stacking log system, seriously.

Fig. \ref{fig:comp_gc_1} illustrates a scenario of compound garbage
collection. We assume that each segment on a filesystem has four pages
and the filesystem performs garbage collection in units of
segments. We further assume that segments that are not depicted in Fig.
\ref{fig:comp_gc_1} are filled with cold data. Each block on a SSD
contains ten pages. The filesystem and the SSD performs garbage
collection when only one empty segments and only one empty block is
available on the layer, respectively. Additionally, we assume that SSD
is aware of the invalidated LBAs through TRIM command.

Each page in the filesystem and the SSD in Fig \ref{fig:comp_gc_1}(a)
keeps the LBA of its respective page, which is denoted as `$Ln$' where
$n$ represents the LBA number, and a flag to note validity of the
page. The flag represents two states of the corresponding page, $V$ is
for valid and $I$ is for invalid page. Let’s further assume that
initially segment 0 holds data pages for LBA 1 to LBA 4, i.e., from
$L1$ to $L4$, and segment 1 contains LBA 5 and LBA 6, i.e., $L5$ and
$L6$. Both segments are stored in block 0 on the SSD. Note that all
the flags on the filesystem and the SSD show that all the data is
valid.

Fig \ref{fig:comp_gc_1}(b) shows the state of the filesystem and the
SSD after LBA 1 and LBA 4 are updated. Since both layers are
log-structured systems, updated data is appended to the end of each
system. The flags in old pages of the filesystem and SSD are now set
to invalid. Upon receiving an update, filesystem writes a new data on
segment 2; the filesystem detects that there is only one segment in
the system, and thus decides to trigger segment cleaning process. 

Fig. \ref{fig:comp_gc_1}(c) shows the status of each layer after the
filesystem segment cleaning process. For example, we assume that the
filesystem selects segment 0 as the victim segment. Valid pages in
segment 0, $L2$ and $L3$, are copied to available empty segment 2. The
filesystem notifies the changes in the system to the storage
device. SSD thinks that $L2$ and $L3$ are updated in filesystem, so it
invalidates pages 1 and 2. Then, SSD writes $L2$ and $L3$ in page 8
and page 9 in block 0, respectively. 

After writing to page 8 and page 9, SSD detects that there is only one
empty block left in the storage system, thus the storage level garbage
collection takes a place, which is illustrated in Fig
\ref{fig:comp_gc_1}(d). Recall that, all the other blocks in the
storage are filled with cold data, thus block 0 is selected as the
victim for the garbage collection.  All the valid pages in block 0 are
copied to empty block 1, and block 0 is erased and becomes a free block.

There are two things to note in our second case study. First, storage
level garbage collection is triggered as a result of filesystem level
garbage collection. Second, filesystem and storage system relocates
pages with $L2$ and $L3$ in the course of garbage collecting segment
0 and block 0, respectively. Recursive garbage collection on the
filesystem and the storage system forces to rewrite the same data over
and over. Such compound garbage collection acts as a factor that
increases Write Amplification Factor (WAF) in SSDs.

As Yang et al. \cite{yang2014don} pointed out in their work, compound
garbage collection problem becomes more serious when garbage
collection unit of higher log system is smaller than the unit of lower
log system. Section \ref{subsec:remove_gc_overhead} provides more
detailed analysis of compound garbage collection scenario.



\section{Design and Implementation}
\subsection{Concept: The Unified Storage Layer}
\label{subsec:concept_USL}

We propose Unified Storage Layer (USL). It has two
major goals. First goal is to reduce the DRAM requirements for large
scale SSDs by minimizing the size of metadata without loss of the
performance. Second goal is to resolve compound garbage collection
problem which arises in stacking log-structured system.

In order to meet the goals, USL takes advantage of recent log filesystem called F2FS
\cite{lee2015f2fs} to both alleviate the heavy memory usage of page
mapping and increase the storage performance. As described in
Fig. \ref{fig:f2fs_partition}, there are two regions in F2FS
partition. And, each area exhibits different write patterns. Since
metadata area handles all writes in in-place update manner, the region
shows random write pattern. On the other hand, data and node in main
area are written in log-structured style which shows sequential write
pattern. Depending on the I/O characteristics of the filesystem, USL distinguishes
two LBA regions on filesystem layer. On one of the regions,
filesystem has control over when to perform garbage collection for 
both filesystem and SSD. Thus SSD does not have to garbage collect 
on its own and also does not need to manage mapping table.


Fig. \ref{fig:system_layout} compares different system layout
including in-place update filesystem (EXT4), log-structured filesystem
(F2FS), and USL. Fig. \ref{fig:ext4_layout} illustrates a
configuration with EXT4 filesystem, which partitions metadata and data
blocks, and each data block has default size of 4 Kbyte. F2FS and USL
shown in Fig. \ref{fig:f2fs_layout} and Fig. \ref{fig:usl_layout},
respectively, manages filesystem partition with segments.

\begin{comment}
  As we have explained in Section \ref{subsec:measuring_waf}, WAF for 
  EXT4 is bounded by $WAF_{SSD}$. And since EXT4 is in-place update in 
  nature, it works best when SSD uses page mapping. Since both layers 
  of F2FS with page mapping SSD suffers from garbage collection, the 
  WAF is bounded by $WAF_{fs}$$\times$$WAF_{SSD}$, and it leads to 
  compound garbage collection problem. On the contrary, WAF of USL 
  can be denoted as $WAF_{fs}$ because filesystem in USL manages the 
  garbage collection behavior of the storage, and it also implies that 
  USL do not suffer from the overhead of compound garbage collection. 
  Additional benefit of using USL is that SSD does not need to maintain 
  a mapping table for main area, which significantly reduced the size 
  of mapping table.
\end{comment}

\begin{comment}
  \subsection{Measuring WAF of Filesystem}
  \label{subsec:measuring_waf}
\end{comment}

The performance of a filesystem or a storage system can be represented
with a notion called Write Amplification Factor, WAF
\cite{rosenblum1992design}, which is defined as a ratio of the amount
of write volume received from a higher level I/O layer and the actual
amount of write volume the lower level I/O layer has to process. 
In general, higher the WAF means
lower the performance because of the volume it has to process. In a
stacking log system, combination of segmentation cleaning and garbage
collection on different layers leads to higher WAF.

\begin{figure*}[t]
\centering
 \subfigure[EXT4 with Page mapping SSD]{
 \includegraphics[width=2in]{./figure/ext4_arch}
 \label{fig:ext4_layout}
 }
 \subfigure[F2FS with Page mapping SSD]{
 \includegraphics[width=2in]{./figure/f2fs_arch}
 \label{fig:f2fs_layout}
 }
 \subfigure[Unified Storage Layer]{
 \includegraphics[width=1.92in]{./figure/usl_architecture}
 \label{fig:usl_layout}
 }
\caption{System Layout for Different Filesystems and SSD Mapping
  Scheme}
\vspace{-0.5em}
\label{fig:system_layout}
\end{figure*}

We provide analytical WAF of each filesystem; EXT4, F2FS, and USL. WAF는
메타데이터 기록 ($W_{meta}$), 저널링 동작 ($W_{journal}$), 그리고 세그먼트 클리닝 
($W_{sc}$)등으로 인해 증가될 수 있다. 먼저, EXT4는 In-place update 파일시스템이므로
가비지 컬렉션은 하지 않지만, 저널 데이터 기록으로 인한 Write Amplification이 발생한다. 
따라서, 사용자가 요청한 쓰기 크기를 $W_{user}$라 할 때, EXT4 파일시스템의 WAF 
($WAF_{ext4}$)는 다음 수식 Eq. (\ref{eq:waf_ext4})과 같이 계산될 수 있다.


\begin{equation}
\label{eq:waf_ext4}
WAF_{ext4} = \frac{W_{user}+W_{meta}+W_{journal}}{W_{user}+W_{meta}}\nonumber
\end{equation}
\vspace{-0.5em}
\begin{equation}
= 1+\frac{W_{journal}}{W_{user}+W_{meta}}
\end{equation}

위의 수식에서, 저널 데이터 쓰기 양이 User 데이터와 메타데이터 쓰기 양보다 
매우 작은 워크로드에서는, EXT4의 WAF는 1에 수렴한다고 볼 수 있다. F2FS와 
USL은 Segment cleaning을 수행하므로, F2FS WAF ($WAF_{f2fs}$)와 USL WAF 
($WAF_{usl}$)는 다음 수식 Eq. (\ref{eq:waf_f2fs})과 같다.

\vspace{-0.5em}
\begin{equation}
\label{eq:waf_f2fs}
WAF_{f2fs} = WAF_{usl} =\frac{W_{user}+W_{meta}+W_{sc}}{W_{user}+W_{meta}}\nonumber
\end{equation}
\vspace{-1em}
\begin{equation}
= 1+\frac{W_{sc}}{W_{user}+W_{meta}}
\end{equation}

각 파일시스템을 SSD 에서 동작시킬 경우, 전체 시스템의 WAF ($WAF_{sys}$)는 
각 파일시스템의 WAF와 SSD의 WAF ($WAF_{ssd}$)를 곱한 값이 된다. EXT4와 F2FS를
사용하는 시스템의 WAF는 각각 다음과 같다.

\vspace{-0.5em}
\begin{equation}
\label{eq:waf_sys_ext4}
WAF_{sys\_ext4} = WAF_{ext4}\times WAF_{ssd} \cong  WAF_{ssd}
\end{equation}
\vspace{-1em}
\begin{equation}
\label{eq:waf_sys_F2FS}
WAF_{sys\_f2fs} =(1+\frac{W_{sc}}{W_{user}+W_{meta}})\times WAF_{ssd}
\end{equation}

EXT4를 사용한 시스템의 WAF는 저널 데이터 쓰기 양이 user 데이터보다 매우 
작다는 가정 하에, SSD의 WAF 값에 수렴한다. 반면 F2FS를 사용한 시스템의 WAF는 
파일시스템의 Segment Cleaning으로 인한 WAF 값과 SSD의 가비지 컬렉션 동작으로 
발생한 WAF 값을 곱한 것이 전체 시스템 WAF 값이 된다. 즉, 파일시스템에서 Segment 
Cleaning으로 인한 쓰기 증폭이 과도할수록, 시스템은 더욱 증가된 WAF 값을 갖게 
된다. 반면 USL의 경우, 메타데이터를 저장하는 매우 적은 SSD 영역에 대해서만 
가비지 컬렉션을 수행하므로, SSD의 WAF값 ($WAF_{ssd}$) 값이 매우 작다. 따라서, 
USL의 WAF는 다음 수식 Eq. (\ref{eq:waf_sys_usl})과 같이 나타낼 수 있다.

\vspace{-0.5em}
\begin{equation}
\label{eq:waf_sys_usl}
WAF_{sys\_usl} = WAF_{usl}\times WAF_{ssd}\cong WAF_{usl}
\end{equation}

다시 말해, USL의 WAF는 파일시스템의 Segment Cleaning으로 인한 쓰기 증폭에만 
영향을 받으며, 이는 implies that USL do not suffer from the overhead of 
compound garbage collection. Additional benefit of using USL is that SSD 
does not need to maintain a mapping table for main area, which significantly 
reduced the size of mapping table.

\begin{comment}
  We provide analytical WAF of an in-place update filesystem, Ext4, and
  a log-structured filesystem, F2FS, to compare the inherent performance
  differences between the two filesystems. 
  WAF를 증가시키는 요인으로는 메타데이터 관련 입출력, 저널링 동작, 그리고
  가비지 컬렉션 등이 있다. We assumed that metadata and journaling
  related I/O volume is significantly lower than that of user data and
  garbage collection related I/O volume and excluded from the computation.
  Overall system WAF is denoted as $WAF_{Total}$, filesystem WAF as $WAF_{fs}$, 
  and storage device level WAF as $WAF_{SSD}$.

  Analytical filesystem WAF of Ext4 is 1 because it is an in-place
  update filesystem. In a storage system with Ext4, the performance
  bottleneck would be garbage collection of the storage device. Thus,
  total WAF of Ext4 system can be represented as Eq. (\ref{eq:waf_ext4}).

  \vspace{-0.5em}
  \begin{equation}
  \label{eq:waf_ext4}
  WAF_{Total}^{Ext4} = WAF_{SSD}
  \end{equation}

  Since segment cleaning on a log filesystem is a must, potentially a
  stacking log system suffers from garbage collection of invalid data on
  all log layers. Thus, the WAF of stacking log system with F2FS can be
  represented as Eq. (\ref{eq:waf_lls}).

  \vspace{-0.5em}
  \begin{equation}
  \label{eq:waf_lls}
  WAF_{Total}^{F2FS} = WAF_{fs}\times WAF_{SSD}
  \end{equation}

  Finally, total WAF of Unified Storage Layer (USL) is bounded by
  filesystem because it dispenses the need of garbage collection in
  storage device. Instead, USL only performs segment cleaning on
  filesystem and delivers list of sections to erase to the
  storage. Total WAF of USL can be represented as Eq. (\ref{eq:waf_usl}).

  \vspace{-0.5em}
  \begin{equation}
  \label{eq:waf_usl}
  WAF_{Total}^{USL} = WAF_{fs}
  \end{equation}

  Eq. (\ref{eq:waf_usl}) implies that 
  USL do not suffer from the overhead of compound garbage collection. 
  Additional benefit of using USL is that SSD does not need to maintain 
  a mapping table for main area, which significantly reduced the size 
  of mapping table.

  Since stacking log system with F2FS runs garbage collection on both
  layers, the system is mostly likely be sensitive to garbage collection
  behavior on each layer and naturally will have higher WAF than
  in-place update filesystems. On the contrary, total WAF of Ext4 only
  depends on garbage collection performance of storage device. However,
  underlying SSD must use page mapping for performance reasons, but the
  fact that there is high cost in using it should not be taken lightly,
  especially when the capacity of device is increasing. In Section
  \ref{sec:CompoundGC}, we measure the effect of processing garbage
  collection on each layers of log system and WAF of different systems.
\end{comment}

\begin{comment}
  In F2FS, the default section size is
  2 Mbyte which is same as the size of a segment. USL, on the contrary,
  uses 256 Mbyte as section size to match the erase unit of Unified
  Flash Storage. All three filesystems translates file offsets into
  logical addresses to access the filesystem partition; Ext4 makes use
  of inode data structure, and F2FS and USL makes use of node data
  structure. I/O requests from each filesystem delivers logical
  addresses to underlying storage, and the address is translated to
  physical addresses by FTL. Using the physical addresses, FTL sends
  program requests to NAND Flash controller. 

  Analyzing WAF of a system allows us to have some idea on the
  performance of the system. As we have explained in Section
  \ref{subsec:measuring_waf}, WAF for Ext4 is bounded by $WAF_{SSD}$ and
  for F2FS is bounded by $WAF_{fs}$$\times$$WAF_{SSD}$. On the contrary,
  WAF of USL is bounded by $WAF_{fs}$ for two reasons. First, the size
  of filesystem metadata is very small compared to the filesystem
  partition---only 1 Gbyte out of 256 Gbyte capacity is allocated as
  metadata area in USL. Second, over-provisioning area available in an SSD
  is large enough to cover all the updates in metadata area. 
\end{comment}

\begin{figure}[t]
\begin{center}
\includegraphics[width=3in]{./figure/usl_layout}
\caption{Disaggregate Mapping Layout}
\label{fig:da_mapping_layout}
\vspace{-2em}
\end{center}
\end{figure}


\begin{table*}[t]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|} \hline
 & USL & ANViL \cite{anvil} & FSDV \cite{zhangremoving} & NVMKV \cite{nvmkv}          & SDF \cite{sdf}      \\ \hline \hline
Host Map           & $\times$         & $\bigcirc$ (prototype)   & $\times$           & $\times$        & $\times$   \\ \hline
Device Map         & $\triangle$ (very small) & $\times$ (prototype) & $\bigcirc$         & $\bigcirc$      & $\bigcirc$ \\ \hline
Mapping Table Size & \# of meta pages & \# of pages & $\leq$ \# of pages & \# of pages     & \# of blocks \\ \hline
GC Layer           & Host             & Device       & Device             & Host            & Host       \\ \hline
New I/O Interface  & $\times$         & $\bigcirc$  & $\bigcirc$         & $\times$        & $\bigcirc$ \\ \hline
SSD OVP            & $\triangle$ (very small) & $\bigcirc$  & $\bigcirc$         & $\bigcirc$      & $\times$ \\ \hline
Target Workload    & General          & General     & General            & Key-Value Store & Specific Server \\ \hline
\end{tabular}
\end{center}
\caption{Comparison between USL and other systems}
\label{tab:compare_usl}
\end{table*}

Table 1은 SSD의 성능을 극대화하기 위해 제안된 최신의 시스템들과 USL을 비교한다. 
ANViL\cite{anvil}은 디바이스의 Logical to Physical 매핑 정보를 수정할 수 있는
인터페이스를 호스트에게 제공한다. 호스트는 이러한 인터페이스를 이용하여, 불필요한 
데이터 쓰기 동작을 제거할 수 있다. FSDV \cite{zhangremoving}는 파일시스템의 inode가 
SSD의 Physical address를 가리키도록 함으로써, SSD의 매핑 테이블 크기를 동적으로 
감소시키는 기법이다. NVMKV \cite{nvmkv}는 key-value store를 다루기 위한 operation 
들을 FTL operation(atomic multiple-block write, p-trim, exists, iterate)로 
대체함으로써, Key-value store의 in-memory metadata를 제거하고 낮은 수준의 
Write Amplification을 갖도록 한다. 마지막으로 SDF (Software-Defined Flash)는 
플래시 메모리의 각 flash channel을 host software가 직접 제어할 수 있도록 함으로써, 
SSD의 over-provision을 제거하고 플래시 메모리의 raw-bandwidth를 응용 프로그램이 
활용할 수 있도록 한다.

이들 시스템의 공통점은 호스트가 SSD의 메타데이터를 직접 관리하거나 수정할 수 있는
권한을 가짐으로써, 성능을 높이거나 \cite{anvil, nvmkv, sdf}, 호스트 또는 디바이스의
메타데이터 크기를 절약한다\cite{zhangremoving, nvmkv}. 이들과 비교하여 USL은,
디바이스의 매핑 테이블 크기를 줄임과 동시에, compound garbage collection으로 인해
발생하는 성능 하락을 방지한다. 더욱이 매핑 테이블 크기가 동적으로 변하는 FSDV와 달리,
USL은 매핑 테이블 크기가 포맷 후 변동되지 않는다. 파일 시스템 메타데이터를 기록하기
위한 SSD 영역에 대해서만 가비지 컬렉션이 동작하므로, 필요한 SSD Over-provisioning
크기가 크지 않으며, 호스트와 SSD간에 새로운 I/O Interface 추가가 필요없다. 마지막으로
특정 워크로드를 타겟으로 하는 NVMKV와 SDF와는 달리, USL은 다양한 워크로드와 시스템을
타겟으로 할 수 있는 장점을 갖는다.

\subsection{Design of Unified Storage Layer}
\label{subsec:design}

\begin{comment}
  Note that F2FS is Flash friendly filesystem and tries to reduce
  storage I/O overhead by sending large units of data. For example, unit
  of segment cleaning for main area is a section which is 2 Mbyte in
  size. The main area does not perform in-place update unless it is
  necessary. The size of metadata and main area are determined by the
  size of the partition, the size cannot be altered after formatting a
  partition.
\end{comment}

\begin{comment}
  After reviewing the filesystem features and the I/O characteristics of
  each area, we came up with an idea to solve the layered garbage
  collections on log system.
\end{comment}

The crux of Unified Storage Layer is to
manage LBAs with disaggregate mapping, which is a mapping specifically
tailored to accommodate two different I/O pattern. In disaggregate
mapping, which is shown in Fig \ref{fig:da_mapping_layout}, metadata
area is managed in page mapping and LBAs in main area are one-to-one
mapped with PBAs in SSD. In other words,
main area of an SSD does not have a mapping table
information, and LBAs, $LBA_i$, are directly mapped with corresponding
PBAs, $PBA_j$, in the SSD, where $i=\{1, \ldots, n\}$ and $j=\{1,
\ldots, n\}$, respectively. SSD Firmware makes decision over
received LBAs. If they are for metadata area, the firmware directs
them to page mapping managed region of the device; if LBAs within the
range of main area is received, the firmware recognizes the LBA as
PBA. 

\begin{comment}
  Note that each section in main area is
  matched with a block in the SSD. Thus, the update in a section is
  applied to the corresponding block in the SSD. By mapping sections to
  physical blocks, that is $section_l \big|_{l=0, \ldots, n} =
  block_{m+1} \big|_{m=1, \ldots, n}$, the FTL does not need to hold
  mapping table for the main area. 
\end{comment}

\begin{translatedtext}{
SSD의 NAND block들은 Superblock 단위로 관리된다. SSD 펌웨어는 쓰기 요청들을
superblock에 기록함으로써, 각 채널, 웨이에 연결된 플래시 메모리를 이용해
병렬적으로 입출력을 처리하게 된다.
}{
NAND blocks in SSD are managed in units of Superblock. 슈퍼블록의
정의가 있는지?
}\end{translatedtext}
Bad block management와 wear-leveling 동작은 SSD의 펌웨어에서 수행한다. 
Bad block management 계층은 파일시스템의 section과 1:1로 매핑되지 않은
space NAND block들을 따로 관리하며, bad block 발생 시 spare NAND block
들로 bad block을 교체한다. 이는 Bad block을 다른 section과 매핑된 
superblock의 NAND block으로 대체할 경우, 가비지 컬렉션으로 인한 superblock 
간의 데이터 복사가 수행될 때, 
bad block에 대한 중복 쓰기가 발생할 수 있기 때문이다. 이러한 문제를 피하기 위하여
bad block management 계층은 파일 시스템의 section과 매핑되지 않은 spare
NAND block으로 bad block을 대체한다.
\begin{translatedtext}{
Wear-leveling은 동작 단위를 Super block 단위로 한다. 이는, USL은 Section 
단위로 가비지 컬렉션을 수행하므로, Section과 매핑된 Superblock에 속한 NAND 
block들의 erase count는 유사하기 때문이다.
}{
In USL, it uses superblock and section as the unit of wear-leveling and 
the unit of garbage collection, respectively, because the erase count
of a section and NAND blocks in a superblock is almost the same.
}\end{translatedtext}

There are at least two significant benefit of using disaggregate 
mapping. First, it has very low memory footprint. Second, it removes 
the overhead of garbage collection in main area of storage in USL, 
which enables USL to avoid compound garbage collection problem.
Note that in order for USL to exploit
disaggregate mapping scheme, filesystem has to know the page and block
size of the SSD in use and SSD has to know the layout of the
filesystem in USL. The negotiation takes in place when the filesystem
is first formatted on the storage device.

\begin{comment}
  There are two layers in USL, that is filesystem
  and Unified Flash storage layer, and rest of this section describes
  each layer in more detail.

  Unlike SSDs with page mapping or other hybrid mapping schemes, USL
  storage device does not keep a mapping information for main area
  of USL filesystem. Unified Flash Storage is defined as the storage in
  USL which uses disaggregate mapping scheme.  There are at least two
  significant benefit of using disaggregate mapping. First, it has very
  low memory footprint. Second, it removes the overhead of garbage
  collection in main area of storage in USL, which enables USL to avoid
  compound garbage collection problem.
\end{comment}

\begin{comment}
  It is combination of page mapping for metadata area and no mapping
  scheme for main area of USL filesystem. By making the size of
  section in filesystem same as the size of NAND block in SSD and each
  section be one-to-one linked to a physical NAND block, it is able to
  eliminate the use of mapping schemes.
\end{comment}

\begin{comment}
  As described in section \ref{subsec:fs_layer}, filesystem
  $section_l\big|_{l=1, \ldots, n}$ is fixed to SSD
  $block_{m+1}\big|_{m=1, \ldots, n}$. SSD Firmware makes decision over
  received LBAs. If they are for metadata area, the firmware directs
  them to page mapping managed region of the device; if LBAs within the
  range of main area is received, the firmware recognizes the LBA as
  PBA, and programs to respective block. 
\end{comment}

\subsection{Implementation}

The filesystem layer in USL plays two important roles. First, it has
to persistently store the user data in the storage device. Second, it
has to handle garbage collection on main area and send set of empty
section numbers acquired from segment cleaning process to USL
storage. Upon receiving the section numbers the device makes the
corresponding NAND blocks as empty blocks. Therefore, there is no need
to garbage collect the NAND blocks belonging to main area but to erase
the target blocks that the filesystem requested.

Four parts of F2FS is modified to meet the requirements of USL
filesystem. First, we introduce patch manager to avoid partial writes and second, 
we modified write policy of the filesystem. Third, we added a mechanism
to transfer section numbers reclaimed by segment cleaning to USL
storage device. and finally, we modified filesystem formatting tool.

\subsubsection{Block patching}

\begin{figure}[t]
\begin{center}
\includegraphics[width=3in]{./figure/patch_manager_ex}
\caption{Two-page Write Behavior with and without Patch Manager}
\label{fig:patch_manager_ex}
\end{center}
\vspace{-1em}
\end{figure}

When SSDs receive a write request that is smaller than the page size
of the SSD, the device writes the data using partial write on a page
which wastes the rest of the page. Note that unit size of F2FS is
4Kbyte, whereas the size of SSDs varies from 4 Kbyte, 8 Kbyte, and to
16 Kbyte, depending on manufacturers. If the unit size of write in
filesystem and SSD are not aligned to each other, there is no other way
but to use partial write on SSD.

Fig. \ref{fig:patch_manager_ex}-(a) illustrates an issue in F2FS, where
SSD uses 8 Kbyte as page size. In the configuration, F2FS allocates a LBA on every 4 Kbyte,
and a SSD block is composed of four 8 Kbyte pages. Each page is
numbered and also has a flag to indicate validity of corresponding
page. Upon receiving a write request for LBA 0 from the filesystem, the
SSD programs it on page 0. Since the page is larger than the size of
request, the SSD partially programs the page with the request and the
rest of 4Kbyte in the page is left empty.

Let’s assume that after sometime, the filesystem sends another
request to write LBA 1. Since a page is 8 Kbyte, LBA 1 can be stored
in page 0 right next to LBA 0; however, due to NAND characteristics
LBA 1 cannot be updated in page 0. If the SSD were to write both LBAs in
one page, LBA 0 in page 0 has to be internally copied to the
buffer and programmed in page 1 with LBA 1. 

Because of the additional read and copy behavior, partial page write 
operation causes the performance degrade of SSDs. 
One way to avoid partial writes in SSD is to format the filesystem
block size to match page size of SSD. However, the available block
sizes in a filesystem is limited. For example, EXT4 supports 1 Kbyte,
2 Kbyte, and 4 Kbyte, and F2FS, on the contrary, cannot change the
filesystem block size. 

\begin{translatedtext}{
F2FS는 NAND page 크기를 4KByte로 가정 후, 
파일시스템 블록 크기를 4KByte로 설정하였으며, 이러한 설정이 파일 시스템의 
모든 메타데이터 설계에 매우 깊히 관여되어 있기 때문에, F2FS는 파일 시스템 블록 크기
변경을 지원하지 않는다.
}{
Instead it assumes 4Kbyte as the size of NAND
page and uses 4Kbyte as the block size to design the filesystem
metadata.
}\end{translatedtext}

More importantly, the partial page write cannot be handled in USL.
As we have mentioned earlier, the main area of the filesystem in USL
does not keep a mapping table and each LBA is directly mapped with
PBA. Thus, every LBAs have its designated location in the USL
storage. For example, LBA 0 and LBA 1 can only be stored in page 0.
Thus, USL cannot handle partial write request such as the workload 
represented in Fig. \ref{fig:patch_manager_ex}-(a).
In order to address the side effect of removing the mapping
information in a storage device, we introduce Patch Manager in USL
filesystem layer to mandate each write requests be aligned with the
page size of the storage. 

The role of patch manager is to forbid write requests that are not
aligned to the page size of a storage in USL and prevent it from
partial writes. Fig. \ref{fig:patch_manager} illustrates patch manager
in the I/O hierarchy. The main role of patch manager is to align the
size of all requests from the filesystem to the page size of the
storage.  Before sending write request to the storage, the patch
manager checks whether the request is aligned with the page. If it is
aligned, then the write request is simply transferred to the storage;
and, if it is not aligned with the page size, then patch manager
allocates an empty page from page cache and concatenates it with
original write request.

\begin{comment}
  USL filesystem manipulates bio structure to form a request for the
  storage. But, before sending it to the storage, the filesystem sends
  it to patch manager to check whether the request is aligned with the
  page. If it is aligned, then it simply returns bio structure; and, if
  it is not aligned with the page size, then patch manager adds a dummy
  page and makes the length of the request aligned with the page size.
\end{comment}

\begin{figure}[t]
\begin{center}
\includegraphics[width=3in]{./figure/patch_manager}
\caption{Block Patch Manager}
\label{fig:patch_manager}
\vspace{-1.5em}
\end{center}
\end{figure}

Fig. \ref{fig:patch_manager_ex}(b) shows how patch manager works in
USL. Before issuing a write request for LBA 0, patch manager checks
whether the request is aligned to 8 Kbyte or not. If it is less than a
page size, patch manager adds dummy 4 Kbyte page along with LBA 0 and
sends down to storage in USL. The storage device assigns LBA 1 as the
dummy data and programs all the data on page 0. After some time, when
the filesystem receives actual write for LBA 1, USL filesystem assigns
it to next logically consecutive address, which is LBA 2. Since the
request for LBA 2 is also not aligned, patch manager also adds dummy
page to the request and writes the request in page 1. Since a dummy
page does not contain any useful data, we mark it as invalid to let
segment cleaning reclaims the page.

With the help of patch manager, USL achieves the goal of mapping each
LBA to PBA in storage systems. However, one may point out a problem of
increased WAF in filesystem layer because of the addition of dummy
page. Our experiments on random workload shows that patch manager
added only 3$\%$ more dummy pages, and we observe that most of writes
in filesystem were in multiples of page size.

\subsubsection{Disable Threaded Logging in Filesystem}

F2FS is hybrid version of log-structured and in-place update
filesystem. When there is enough space in the partition, F2FS appends
all the writes; however, when the available space goes under certain
threshold, F2FS uses threaded logging \cite{oh2010optimizations}, which in-place
updates a new data on invalid blocks on the filesystem. F2FS
exploits threaded logging to delay segment cleaning from happening and also reduces
WAF in filesystem layer.

However, threaded logging in F2FS means in-place update
with random write on sections in main area.
Note that LBAs in main area of USL filesystem are directly
mapped with PBAs. Thus threaded logging
feature of the F2FS makes the filesystem to randomly update data
which then forces SSDs to program NAND pages with random offset.  
But Flash memories prohibit random page address programming and
imposes that the NAND pages must be programmed consecutively \cite{samsung_Flash}.
In order to guarantee that writes to main area are with only sequential writes, 
we disabled the use of threaded logging in USL filesystem.

Although threaded logging generates random writes, one benefit of using
threaded logging is that it suppresses segment cleaning
operations. Note that disabling threaded logging USL causes it to
perform segment cleaning repeatedly when the filesystem is in high
utilization. We compared the performance of USL and threaded logging
in a filesystem under high utilization (Section
\ref{subsec:remove_gc_overhead}) and show that garbage collection
policy of USL outperforms F2FS with threaded logging. 

\subsubsection{Delivering filesystem format information}

In Section \ref{subsec:design}, we described that layers in USL needs
to go through a negotiation phase to inform the capacity, the size of
page, and the erase unit of the storage to the filesystem, and to define
the regions for metadata and main area of the filesystem on the
storage. The session is completed in three steps: (i) At the beginning
of filesystem format, USL device acknowledges with its page, erase unit, 
and storage capacity to the filesystem, (ii) the filesystem sets the 
size of a section, creates metadata and main area, and returns the area 
information to USL storage device, and (iii) the storage device 
initializes metadata area with page mapping table and let main area be 
managed by the filesystem. 

We modified f2fs-tools \cite{f2fs_tools} to acquire the
information and exploit them in USL, and added fields to store erase
unit and the size of the page of the storage in f2fs-tools.

\begin{comment}
  Theses information is transferred to each other at filesystem
  format time.   
  The size of metadata in USL filesystem depends on the capacity of the
  storage. As soon as the capacity is made known to the filesystem,
  it creates a filesystem partition and passes down the region for
  metadata and main area to the storage

  In the metadata area, F2FS keeps superblock which holds filesystem
  partition information,
  checkpoint for filesystem recovery, segment information table that
  records validity and other information about segments, and node
  address table that keeps account of file related metadata. 
  After formatting, USL filesystem has segment cleaning unit aligned with
  garbage collection unit of the storage, and patch manager can send
  page aligned write requests to the storage.
\end{comment}

\begin{comment}
  \begin{figure}[t]
  \begin{center}
  \includegraphics[width=3.2in]{./figure/preemptive_sc}
  \caption{Quasi-Preemptive Segment Cleaning Behavior}
  \label{fig:quasi_sc}
  \vspace{-1.5em}
  \end{center}
  \end{figure}
\end{comment}

\begin{figure}[t]
\centering
 \subfigure[Non-preemptive Segment Cleaning]{
 \includegraphics[width=3.4in]{./figure/preemptive_sc_1}
 \label{fig:non_preemptive}
 }\hspace{-1.3em}
 \subfigure[Preemptive Segment Cleaning]{
 \includegraphics[width=3.4in]{./figure/preemptive_sc_2}
 \label{fig:preemptive}
 }
 \caption{Quasi-Preemptive Segment Cleaning Behavior}
 \vspace{-1.5em}
 \label{fig:quasi_sc}
\end{figure}

\section{Resolving the Segment Cleaning Overhead}

\subsection{Garbage Collection based Segment Cleaning}

Because the filesystem layer of USL takes charge of garbage 
collection for the main area, the result of segment cleaning 
decides which NAND block should be erased. After the filesystem 
selects victim sections and performs segment cleaning, it sends 
the section numbers to device. Upon receiving section numbers from the
filesystem, the storage device needs to just erase blocks

\begin{translatedtext}{
USL 파일시스템은 segment cleaning의 결과로 확보된 section을 
free section으로 바로 환원하지 않고, pre-free 상태(해당 section 내의
데이터가 모두 invalidation된 상태)로 남겨둔다.
Pre-free 상태의 sections를 free sections로 환원하는 시점은 파일시스템이
checkpoint를 수행할 때 이며, 또한 해당 시점에 새 free section의 
번호들을 device로 전달한다. 즉, 파일시스템은 디바이스가 section 번호에 
해당하는 NAND block들을 erase 하도록 한 뒤, 해당 section을 free section
으로 환원하여, 추후 쓰기 요청을 처리한다.
}{
When USL filesystem completes segment cleaning, it does not put
reclaimed section to free section; instead, it leaves the section as
pre-free section which only holds invalidated data. All pre-free
sections are put into free sections on filesystem checkpoint, and the
new free section numbers are delivered to the device. Upon receiving
the free section numbers, the device erases the corresponding NAND
blocks and put the section into free section to serve incoming I/O
requests.
}\end{translatedtext}

USL requires a means to transfer the acquired set of section
numbers from filesystem segment cleaning process to the storage
device. We used SATA write command to send section numbers.
To distinguish ordinary write calls from sending section numbers, 
we used SATA command extension. The payload of the command has
new free section numbers.
When SSD receives the SATA command extension, the firmware of SSD recognizes
the write call as a mean to inform section numbers to erase. Upon receiving 
section numbers from the filesystem via write system call, the storage 
device needs to just erase blocks. 

\begin{comment}
  Since all PBAs are matched to LBAs of main area of USL
  filesystem, the storage device does not perform garbage collection on
  those blocks; instead, the filesystem performs segment cleaning and
  sends the section numbers that needs to be erased from the storage.
\end{comment}

\subsection{Quasi-Preemptive Segment Cleaning}
The unit of segment cleaning in USL is section, and we configure it to
match the erase unit of SSD which is the size of NAND block. Thus, as
the size of NAND block increases, USL has to perform segment cleaning
in larger sections. For example, SSD used in the experiment has
superblock of 256 Mbyte and the section in filesystem is set to
256 Mbyte. Note that larger section size means longer response time to
reclaim a victim section to a free section. We propose
Quasi-Preemptive Segment Cleaning scheme to reduce the response time
of segment cleaning.

Quasi-Preemptive Segment Cleaning allows preemption of segment
cleaning by user I/O requests. Since a section is composed of multiple
segments, the scheme checks whether there is outstanding host user I/O
request after each successful segment cleaning of a segment in the
section. If there is pending user I/O. Quasi-Preemptive Segment
Cleaning preempts current segment cleaning operation and serves the
host user I/O. Fig. \ref{fig:quasi_sc} illustrates an example of
Quasi-Preemptive Segment Cleaning with a victim section. There are
four segments, A0 to A3, in the section. While segment cleaning segment
A1 the system received host user I/Os. Upon completion of segment
cleaning, Quasi-Preemptive Segment Cleaning checks for any outstanding
I/Os. Since there is a write request pending,
the system preempts current segment cleaning operation and serves the
write request. When the request is completed, the system resumes preempted
segment cleaning operation and cleans segment A2 and A3.

When filesystem utilization is not high, the system can benefit from
using Quasi-Preemptive Segment Cleaning because it provides better
response time for larger sections. However, when the utilization is
high then the segment cleaning operation should be non-preemptive,
which is left as future work in USL.

\begin{comment}
\section{Write Amplification Analytic Model}
\label{sec:WA_model}


%5장의 존재 의미
Write amplification (WA) measures the the number of user page writes
against actual number of page writes. Since SSDs are out-of-place
update device, WA can describe the performance of the
device. Through modeling of WA of SSDs, we try to understand the
theoretical performance of the device and use it to analyze the
bottleneck in the system. 


%기존의 WAF analytic model
There are number of works in the field to provide analytical model of
SSD write amplification \cite{Hu:2010:RZ3771,
  Hu:2009:WAA:1534530.1534544, Agarwal5700261, Luojie6167472,
  Desnoyers:2012:AMS:2367589.2367603, VanHoudt2013}. Hu et
al. \cite{Hu:2010:RZ3771} used the coupon collector's problem to model
random write behavior of SSDs and to derive write
amplification. Agarwal et al. \cite{Agarwal5700261} derived a model
based on the assumption that uniform number of invalid pages are in
all of the blocks if random workload is uniformly distributed across
the device. Luojie \cite{Luojie6167472} made elaborations to the work
of Agarwal et al. \cite{Agarwal5700261}, and found the probability of
page invalidation and used it to find the number of invalid pages in a
block. Desnoyers \cite{Desnoyers:2012:AMS:2367589.2367603} improved
the model using Markov chain and Van Houdt used mean field model to
add accuracy in modeling the WAF. 


%사용한 모델, 가정한 내용
In this paper, we used a model proposed by Desnoyers
\cite{Desnoyers:2012:AMS:2367589.2367603} to derive the WAF of given
systems, and also used uniformly distributed traffic and greedy
cleaning model which best describes our experiment environment. Note
that provided modeling disregards the effect of wear-leveling and
interaction between channels and ways of a SSD. The unit of write in
this model is a page. Taking account of provided assumptions, the
write amplification can be formulated as
Eq. (\ref{analy_grd_unif_wa}), where $n_p$ denotes number of pages per
block and $X_0$ is defined as Eq. (\ref{analy_grd_unif_X0_rho}). 


%수식 작성 및 인자설명
\begin{equation}
\label{analy_grd_unif_wa}
A=\frac{n_p}{n_p-(X_0-1)}
\end{equation}


%alpha
%\begin{equation}
%\label{analy_grd_unif_X0}
%X_0=\frac{1}{2}-\frac{n_p}{\alpha}\mbox{W}\left( -(1+\frac{1}{2n_p})\alpha e^{-(1+\frac{1}{2n_p})\alpha}\right)
%\end{equation}

%rho
\begin{equation}
\label{analy_grd_unif_X0_rho}
X_0=\frac{1}{2}-\frac{n_p}{\rho+1}\mbox{W}\left( -(1+\frac{1}{2n_p})(\rho+1)e^{-(1+\frac{1}{2n_p})(\rho+1)}\right)
\end{equation}

$\mbox{W}()$ in Eq. (\ref{analy_grd_unif_X0_rho}) describes Lambert W
function, which is commonly understood as the inverse function of
$f(x) = x e^{x}$ and used to find solutions to transcendental
equations \cite{Corless:BF02124750}. Over-provisioning factor of the
device is denoted by $\rho$, which describes the relationship between
the number of physical blocks, $T$, and number of user blocks, $U$,
that is $\rho=\frac{T-U}{U}$. 

\end{comment}

\vspace{-0.5em}
\section{Experiment}

In this section, we compare the size of the mapping tables of different FTL schemes and we
show that disaggregate mapping in USL reduces the size of the mapping
table significantly. We also analyze the compound garbage collection
issue discussed in Section \ref{sec:CompoundGC}, and show that USL
can be a solution to the problem.

\subsection{Experiment Setup}
\label{subsec:exp_setup}

\begin{comment}
  \begin{table}[h]
  \begin{center}
  \begin{tabular}{|c|c|c|c|} \hline
  		     & F2FS	& Ext4	& USL 		\\ \hline\hline
  Filesystem	& F2FS	& Ext4	& Unified FS	\\ \hline
  SSD Mapping	& Page	& Page	& Disaggregate	\\ \hline
  \end{tabular}
  \end{center}
  \caption{System Information (Filesystem and SSD Mapping Scheme)}
  \label{tab:system_info}
  \end{table}
\end{comment}

We compare the performance of USL with base F2FS and in-place update
filesystem, Ext4, on Linux Kernel 3.18.1. USL uses disaggregate
mapping, and base F2FS and Ext4 uses page mapping SSD. 
We used 
Samsung SSD 843Tn\cite{ssd843tn} for the experiment, and modified its
firmware to implement USL. Table \ref{tab:ssd_info} shows
specification of host system and SSD 843Tn used in the performance
evaluations. The total available capacity of the SSD is 256 Gbyte with
23.4 Gbyte over-provisioning area and with 8 Kbyte page. The SSD performs
garbage collection in units of superblock with size of 256 Mbyte where
superblock is group of Flash blocks with same way number in an array of
Flashes channel.

% Table \ref{tab:system_info} summarizes the system information. 
\begin{comment}
  Since SSDs are sensitive to test environment, we create a precondition
  prior to performing any experiments. Detailed steps in the preparation
  is described in each experiments. 
\end{comment}

\begin{table}[t]
\begin{center}
\begin{tabular}{|c|l|} \hline
\multirow{4}{*}{System} & CPU:  Intel i7 @3.40GHz \\ 
& Memory:  8 Gbyte					\\ 
& OS: Ubuntu 14.04					\\ 
& Kernel: 3.18.1 ver				\\ \hline
\multirow{4}{*}{Storage} & Capacity: 256 Gbyte \\
& Over-provisioning: 23.4 Gbyte		\\ 
& Page size: 8 Kbyte				\\ 
& Block size: 4 Mbyte				\\ \hline
\end{tabular}
\end{center}
\caption{Host system and storage (Samsung SSD 843Tn \cite{ssd843tn}) used 
in experimentation.}
\vspace{-1.5em}
\label{tab:ssd_info}
\end{table}

\subsection{The Size of Mapping Information}


\begin{table}[t]
\begin{center}
  \begin{tabular}{|c|r|} \hline 
                         & Mapping Size       \\ \hline\hline 
Page mapping             & 256 Mbyte          \\ \hline 
FSDV\cite{zhangremoving} & $\leq$  256 Mbyte  \\ \hline 
Hybrid mapping (LAST \cite{last08}) & 4 Mbyte \\ \hline 
Disaggregate Mapping     & 1 Mbyte            \\ \hline
\end{tabular}
\end{center}
\caption{The Size of Mapping Table (256 Gbyte SSD)}
\vspace{-0.5em}
\label{tab:meta_size}
\end{table}

Table \ref{tab:meta_size} compares the size of mapping tables in
different mapping schemes when SSD in Table \ref{tab:ssd_info} is used.
Page mapping uses 256 Mbyte of memory when disaggregate mapping uses
only 1Mbyte. Although page mapping has large memory footprint, its
most valuable characteristics is that data can be placed in any
available places in the storage. However, as the size of SSDs is
increasing, merits in using page mapping are becoming less
appealing. For example, for 1 Tbyte and 4 Tbyte SSD with 4 Kbyte as
the page size, the memory space required to store the mapping table
information is 1Gbyte and 4Gbyte, respectively.

File System De-Virtualizer, FSDV \cite{zhangremoving}, makes
filesystem point to a physical address in a SSD, and the pointed entry
in the SSD is removed from the mapping table. Thus, the size of the
mapping table is dynamically resized. In the worst case scenario, it
has to maintain 256 Mbyte of mapping table, just like the page mapping
table. 

\begin{comment}
  다른 예로, VFSL(Virtualized Flash Storage Layer
  \cite{josephson2010dfs})는 Logical to Physical 매핑 정보를 SSD의
  FTL이 아닌, 호스트의 Device Driver 단에서 관리하는 기법이다. VFSL은
  호스트에서 매핑 정보를 관리하므로, 호스트의 CPU, 메모리 자원을 활용할
  수 있는 장점을 갖으나, 매핑 테이블 크기는 이득이 발생하지 않으므로,
  여전히 큰 매핑 정보 관리 overhead가 발생한다. FSDV와 VFSL에 대한
  자세한 설명은 \ref{related_works}에 기술되어 있다.
\end{comment}

Disaggregate mapping uses page mapping for the region allocated for
metadata area of filesystem and keeps no mapping information for the
main area. The memory footprint of USL is only 1 Mbyte which consumes
about 256 times less than that of page mapping. Even if we add several
other metadata used by USL, such as segment bitmap and buffered
segment number, the size of total metadata is only 4.73 Mbyte, which
is 54 times less than that of page mapping. Note that the size of
mapping table without any other metadata in LAST FTL is 4 Mbyte.

\begin{comment}
  Since the size of metadata in disaggregate mapping does not change
  once the partition size is fixed, it makes possible to store the
  information in small DRAM memory.
\end{comment}


\subsection{IO Performance}
\label{subsec:io_performance}

In this section, we measure the performance of different systems where
filesystem does not copy valid data through segment cleaning
process. Fig. \ref{fig:benchtest} shows the performance of sequential
write and random write on each layout. 

\begin{figure}[t]
\centering

 \subfigure[Sequential Write]{
 \includegraphics[angle=-90,width=1.55in]{./bench/seq_write}
 \label{fig:benchtest_seqw}
 }\hspace{-1.3em}
 \subfigure[Random Write]{
 \includegraphics[angle=-90,width=1.55in]{./bench/rand_write}
 \label{fig:benchtest_randw}
 }
 \caption{Sequential / Random Write Performance, F2FS and Ext4 are on
   Page mapping SSD (Workload is as follows. Sequential Write: 188
   Gbyte File size, 512 Kbyte record size, 2.75 Tbyte Total write
   volume / Random Write: 50 Gbyte File size, 4 Kbyte record size, 750
   Gbyte Total write volume, Section size of F2FS: 256 Mbyte)}
 \vspace{-0.5em}
 \label{fig:benchtest}
\end{figure}

Note that Ext4 and F2FS are mounted on page mapping SSD and USL uses
disaggregate mapping SSD with modified F2FS. Details of environment is
described in Section \ref{subsec:exp_setup}. To measure the
sequential write performance, we format the filesystem and create a
file with size of 188 Gbyte. One iteration of an experiment issues 512
Kbyte buffered sequential write until all LBAs are covered, and
repeat the iteration for fifteen times.
Fig. \ref{fig:benchtest_seqw} shows the average performance. The
performance of Ext4 and F2FS is 466 Mbyte/sec and 476 Mbyte/sec,
respectively. However, the performance of USL shows about 507
Mbyte/sec which is about 6$\%$ higher than that of other
filesystems. USL shows better performance because address
translation overhead is removed in USL. 

The performance gap between ext4 and USL stands out even
more in random write workload, which is shown in
Fig. \ref{fig:benchtest_randw}. To measure the random performance of
the device, we format the device and create a 50 Gbyte sized file in
the partition. An iteration of the experiment touches all the LBAs
with 4Kbyte buffered random writes, and the graph shows average of
fifteen iterations. The result shows that USL is about 12$\%$ faster
than Ext4; IOPS of USL is 110.3 KIOPS and Ext4 is 98.1 KIOPS. 

\begin{comment}
  \begin{figure}[t]
    \begin{center}
    \includegraphics[width=3.3in]{./bench/mobibench_randw_iops.eps}
    \caption{Mobibench 4 Kbyte Random Write Test (170 Gbyte Cold data,
      20 Gbyte Hot data, Section size of F2FS: 256 Mbyte)}
    \label{fig:mobibench_randw}
    \vspace{-1.5em}
    \end{center}
  \end{figure}

  Fig \ref{fig:mobibench_randw} shows average IOPS of fifteen iterations of
  random write I/O generated by Mobibench benchmark tool
  \cite{jeong2013androstep}. After formatting the partition, we create
  a 170 Gbyte sized file as a cold data, and create another 20 Gbyte
  sized file as a hot data. An iteration of experiment writes 20 Gbyte
  file with 4 Kbyte buffered random write. This workload also shows no
  sign of garbage collection because free space available in the
  filesystem partition is larger than the hot data we used in the
  experiment. The result shows that IOPS of F2FS (80.5 KIOPS) and Ext4
  (81.0 KIOPS) is not much different from each other. On the other hand,
  USL is slightly faster than the other systems, it shows about 8$\%$
  higher random write IOPS (86.8 KIOPS).
\end{comment}

\begin{figure}[t]
  \begin{center}
  \includegraphics[width=2.9in]{./bench/filebench.eps}
  \caption{Filebench Test Results}
  \label{fig:filebench}
  \end{center}
  \vspace{-1.5em}
\end{figure}

Fig \ref{fig:filebench} shows the result of filebench using fileserver
and varmail as the workload. Note that the performance is normalized
with respect to the performance of EXT4. In the case of fileserver,
EXT4 shows 225 Mbyte/sec where F2FS and USL exhibits 350 Mbyte/sec and
352 Mbyte/sec, respectively. USL shows 1.6 times better performance
than EXT4. The result is similar in varmail. The performance of USL is
29 Mbyte/sec and EXT4 is 20 Mbyte/sec which is 1.5 times higher.

\begin{translatedtext}{
이러한 테스트 결과는, 작은 크기의 discard command를 보내는 Ext4에 비해,
segment 단위의 discard command를 보내는 F2FS와 USL에서 discard processing overhead가
적었기 때문으로 보여진다.
}{
-- 성능 결과 그림에 min max를 표시하면 더 좋겠음 --
The
performance difference comes from the size of discard command. --
구체적으로 discard 크기 분포 차이를 명시하면 좋겠음 -- EXT4 sends a
lot of small sized discard commands but F2FS and USL sends at least
segment sized discard commands which significantly reduces the
overhead of processing discard commands in the device. 
}\end{translatedtext}

\subsection{재실험 필요: Removing the Garbage Collection Overhead}
\label{subsec:remove_gc_overhead}

Section \ref{sec:CompoundGC} described case studies where compound
garbage collection becomes a problem. Based on the studies, we measure
the performance of USL and compare it with F2FS. Note that compound
garbage collection is a problem only exists in stacking log
system. And, to give a fair trial between the log-structured
filesystems, we only compare the result of F2FS and USL. Precondition
of experiments is as follows. We format the device and create a
partition using available 256 Gbyte of space; Create 170 Gbyte
file using sequential buffered write and flushed the dirty
pages in page cache using fsync() system call. Single iteration of
experiment writes 85 Gbyte of created file starting from 0 to 85 Gbyte
LBAs of the file with 4 Kbyte buffered random write. We repeat the
iteration fifteen times and measure the WAF and the performance of
each iteration. We used smartmontools package\cite{smartmontools} to get the WAF of storage.
The result shown at Fig. \ref{fig:f2fs_vs_usl} is
the average of fifteen iterations.
%Fig. \ref{fig:f2fs_vs_usl} shows the result.


\begin{figure}[t]
  \centering
  \subfigure[WAF]{
    \includegraphics[width=2.1in]{./comp_gc/f2fs_vs_usl_waf}
   \label{fig:f2fs_vs_usl_waf}
   }
   \subfigure[IOPS]{
   \includegraphics[width=1.1in]{./comp_gc/f2fs_vs_usl_iops}
   \label{fig:f2fs_vs_usl_iops}
   }
   \caption{The Result of Compound Garbage Collection Scenario 2
     (Section size of F2FS: 256 Mbyte)\label{fig:f2fs_vs_usl}}
\vspace{-1.5em}
\end{figure}

\begin{comment}
\begin{figure*}[t]
\label{fig:170_85_randw}
\centering

\subfigure[Filesystem WAF]{
 \includegraphics[width=3.3in]{./comp_gc/170_85_randw_fs}
 \label{fig:170_85_randw_fs}
 }
\subfigure[Device WAF]{
 \includegraphics[width=3.3in]{./comp_gc/170_85_randw_dev}
 \label{fig:170_85_randw_dev}
 }
\subfigure[Total WAF]{
 \includegraphics[width=3.3in]{./comp_gc/170_85_randw_total}
 \label{fig:170_85_randw_total}
 }
 \subfigure[IOPS]{
 \includegraphics[width=3.3in]{./comp_gc/170_85_randw_iops}
 \label{fig:170_85_randw_iops}
 }
 \caption{The Result of Compound Garbage Collection in Case Study 2
   (Section size of F2FS: 256 Mbyte)\label{fig:170_85_randw}}
\end{figure*}
\end{comment}

Fig. \ref{fig:f2fs_vs_usl_waf} shows the WAF observed on filesystem and
device along with the total WAF, which is product of filesystem WAF
and device WAF. The result shows that using USL increases filesystem
WAF by 38\% compared to that of F2FS with page mapping. On the other
hand, the WAF of device shows that USL is 56\% better than existing
F2FS. The total WAF of USL is 39.3$\%$ lower than that of
F2FS. Fig. \ref{fig:f2fs_vs_usl_iops} shows IOPS of each
configuration. As a result of keeping the overall WAF low, USL
achieved about 77.8\% better IOPS than F2FS.

Fig. \ref{fig:io_distribution} illustrates the write volume generated
for user data, filesystem segment cleaning, and device garbage
collection. An iteration of random write with F2FS generates total of
215 Gbyte of write requests to storage, but USL generated only 131
Gbyte of write requests to storage, which is about 40$\%$ lower. It is
important to note that the difference in volume of extra writes for
garbage collection in each layer. Although segment cleaning in F2FS
generated 10 Gbyte of volume, compound garbage collection forced SSD to
write 120 Gbyte more on the storage device. On the other hand, USL
writes only 46 Gbyte more for segment cleaning process. The reason USL
has larger volume in segment cleaning is that F2FS processed most of
the writes with SSR--average of 80 Gbyte of volume is written using
SSR. Although filesystem WAF of F2FS is about 30$\%$ lower than USL,
the device WAF is about twice larger on the average.



\begin{figure}[t]
\begin{center}
\includegraphics[width=1.5in]{./comp_gc/io_distribution.eps}
\caption{Write Volume Distribution}
\label{fig:io_distribution}
\vspace{-1.5em}
\end{center}
\end{figure}


\begin{figure}[t]
\begin{center}
\includegraphics[width=3.2in]{./comp_gc/cold_ratio_iops.eps}
\caption{Random Write Performance according to the Cold Data Ratio (The total size of cold data and hot data is 170 Gbyte, Section size of F2FS: 256 Mbyte))}
\label{fig:cold_ratio_iops}
\vspace{-1.5em}
\end{center}
\end{figure}

Fig. \ref{fig:cold_ratio_iops} shows the result of 4 Kbyte buffered
random writes on different systems while varying the size of cold data
from 50$\%$ to 95$\%$ of 170 Gbyte in the filesystem partition. The rest
of 170 Gbyte is filled with hot data. For example, when cold ratio is
set to 60$\%$, the size of hot data is set to 40$\%$ and the size is
102 Gbyte and 68 Gbyte, respectively. The result shows the average of
fifteen iterations. 

In all cases, USL outperforms the result of F2FS; USL is about 40$\%$
faster than F2FS when the cold ratio is 50$\%$ and the difference
closes in to 4$\%$ when the cold ratio becomes 95$\%$. It shows that the
effect of compound garbage collection becomes less significant as the
hot ratio becomes smaller. The performance of Ext4 is stable around 74
KIOPS. The performance of USL becomes higher than Ext4 when the cold
ratio is less than 65$\%$ and the gap widens as the ratio
decreases. When the cold ratio set to 95$\%$, USL shows about 25$\%$
higher IOPS than that of Ext4. Considering the report that the size
of the hot data in the system is about 5 to 25 $\%$ of the workload
\cite{park2011hot}, it is reasonable to say that the
performance of USL is superior to other systems, especially when most
of the partition is filled with cold data and only small amount of
data is frequently accessed. Improving the performance when the cold ratio is
lower than 65$\%$ is left as future work.   

The experiments in this section can be summarized in two. First,
compound garbage collection issue described in Section
\ref{sec:CompoundGC} is real and leads to poor performance. Second,
proposed USL with disaggregate mapping allows not only reducing the
device level garbage collection but also lowering the total WAF of the
system.

\subsection{Effect of Segment Cleaning Unit}


Increase in WAF for compound garbage collection means one thing:
misalignment of garbage collection unit between storage and
filesystem. To illustrate the effect of misalignment,
Fig. \ref{fig:segs_per_sec} shows the performance of F2FS while
varying the size of section, which is the unit of segment cleaning in
F2FS. For comparison, we used same condition and workload used in
Fig. \ref{fig:f2fs_vs_usl}. The size of section is increased from 2
Mbyte to 256 Mbyte in multiples of two. X-axis of the graph shows the
size of section and used system. Y1-axis and Y2-axis shows IOPS and
WAF of experiments, respectively. The result shows that as section
size increases the performance also increases, and when the size of
section matches the garbage collection unit of the storage device the
performance is the highest. The performance of USL is about 77.8$\%$
higher than F2FS because section size is one to one aligned with
superblock of the storage device.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.7in, angle=-90]{./comp_gc/segs_per_sec.eps}
\caption{Random Write Performance According to the Number of Segments
  per Section}
\label{fig:segs_per_sec}
\vspace{-1.5em}
\end{center}
\end{figure}

The result section size experiment conveys two things. First, it is
important to match the garbage collection unit between filesystem and
storage device to reduce the garbage collection overhead in layered
log system. Second, it is also better to match the layout of the
two log systems, that is to use one-to-one mapping between filesystem
and storage device.

\begin{comment}
  \subsection{Multi-threaded Write}

  Fig. \ref{fig:50_50_seqw_waf} shows the Total WAF---filesystem WAF
  $\times$ device WAF---of F2FS and USL while processing multi-threaded
  sequential update workload, which is described in Section
  \ref{subsec:case_study_3}. After formatting the partition, we create
  fifty 3.8 Gbyte files sequentially. Then, create fifty threads to
  write 0 to 1 Gbyte range of each file with 256 Kbyte buffered
  sequential update operation, which is one iteration of the experiment.

  Fig. \ref{fig:50_50_seqw_waf} shows the WAF of seven runs. Except the
  first iteration, USL shows about 15$\%$ lower WAF compared to the
  system with base F2FS. It shows that WAF of F2FS is in between 1.5 to
  2. As we have discussed in Section \ref{subsec:case_study_3}, although
  each thread issues sequential write requests, the group behavior of
  multi-threaded I/O requests may increase the overall WAF of the
  system. In the case of USL, filesystem triggers number of segment
  cleaning jobs, but since there is no device level garbage collection,
  USL shows WAF of 1.3 to 1.4, overall USL shows about 26$\%$ lower WAF
  than that of F2FS. 

  In this section, we observed that stacking log-structured system
  suffers from compound garbage collection, and it also showed that
  without proper coordination between the two systems, stacking log
  system is bounded to have high write amplification. Unified Storage
  Layer is combination of efforts to reduce the size of mapping table
  using disaggregate mapping and resolve compound garbage collection by
  delegating SSD level garbage collection to filesystem. And experiments
  in this section show that it successfully addresses the problem.
\end{comment}

\begin{comment}
  match the garbage collection unit between
  filesystem and storage device to resolve compound garbage
  collection. And experiments in this section shows that it successfully
  address the problem.
\end{comment}

\begin{comment}
  \begin{figure}[t]
  \begin{center}
  \includegraphics[width=3.5in]{./comp_gc/50_50_seqw_total}
  \caption{The Result of Compound Garbage Collection Scenario 3 (Section
    size of F2FS: 256 Mbyte)}
  \label{fig:50_50_seqw_waf}
  \vspace{-1.5em}
  \end{center}
  \end{figure}
\end{comment}

\section{Related Works}
\label{related_works}

Yang et al. \cite{yang2014don} illustrated the effect of stacking a
log-structured layer on top of another log-structured layer, i.e.,
using log-structured filesystem on top of SSD. They pointed out that
although using log-structured filesystem provides benefit of increased
write performance and also provides useful feature such as snapshot,
garbage collection on each layer reduces the life of SSD. After
log-on-log simulation based on F2FS, they showed that it is better to
keep the size of upper segment larger or equal to the size of
lower segment and perform upper layer garbage collection before lower
layer garbage collection yields better performance.

SSD의 성능을 극대화하기 위해 Host가 logical to physical 매핑 정보를 직접
관리하거나, 디바이스의 매핑 테이블을 접근하여 수정하는 기법도 제시되고 있다. 
ANViL\cite{anvil}은 디바이스의 Logical to Physical 매핑 정보를 수정할 수 있는
인터페이스를 호스트에게 제공한다. 호스트는 제공된 인터페이스를 
이용하여 Single Journaling, File snapshot 등을 수행하고 불필요한 쓰기 동작을
제거할 수 있으며, 중복제거 동작을 수행할 수 있다. NVMKV \cite{nvmkv}는 key-value 
store를 다루기 위한 operation 들을 FTL operation(atomic multiple-block write, 
p-trim, exists, iterate)로 대체하였다. 즉, SSD의 mapping table을 Key-value 
store metadata로서 사용하고, host의 in-memory metadata를 제거하였다. 
NVMKV은 이러한 방식으로 메타데이터의 중복 관리 overhead를 제거하고, 
낮은 수준의 Write Amplification을 갖도록 하였다.

Zhang et al. \cite{zhangremoving} introduced FSDV (File System
De-Virtualizer) to reduce the memory overhead of managing mapping
information on both filesystem and storage device. It is a user-level
tool that becomes active when either the system becomes idle or system
memory is depleted due to increase in mapping information. When FSDV
is invoked, it first checks mapping information and makes filesystem
to point the physical address and removes logical to physical address
mapping information on SSD mapping table. Their approach reduced the device mapping
information to about 75\% at best. However, the worst case scenario
forces to store all the logical to physical mapping information. One
of the downside of using FSDV is that the memory on the device cannot
be smaller than the maximum size of the mapping table.

SDF (Software-Defined Flash)는 Baidu에서 개발한 자사 서버용 
스토리지 시스템으로, sequential 쓰기가 대부분을 차지하며, Temporal 
locality가 없는 특정 서버 워크로드를 대상으로 하는 시스템이다. 플래시 
메모리의 각 flash channel을 host software에게 개별 storage Device로 
보여줌으로써, host의 각 프로세스가 채널을 점유하여 사용할 수 있도록 하였다.
또한 host가 플래시 메모리의 erase 동작을 담당하게 함으로써, SSD의 Over
-provisioning 영역을 제거할 수 있었다. 이러한 기법을 통해 SDF는 스토리지
스토리지 raw bandwidth의 95\%, raw capacity의 99\%를 활용할 수 있도록
하였다.

Josephson et al. \cite{josephson2010dfs} introduced Direct File System
(DFS) which tries to exploit the maximum performance of NAND Flash
Memory. They pointed out that mix of various complex techniques such as
block allocation policy, buffer cache, and crash recovery, made
filesystem operations too complicated and hinder the performance of
underlying storage. They implemented Virtualized Flash Storage Layer
(VFSL) in device driver layer which replaces the role of block
management and FTL. VFSL keeps mapping information between virtual
address and physical address in Flash memory, and also takes care of
garbage collection and wear leveling of the device. DFS is a
filesystem that exploits VFSL to read and write the Flash memory. VFSL
is implemented for Fusion IO. The design of DFS is greatly simplified
because VFSL manages block allocation and Inode management that used
to be manipulated in filesystem. Note that VFSL takes responsible for
all the operation. Thus, it does not suffer from compound garbage
collection problem. Since VFSL manages virtual to physical address
mapping information, it does not have much benefit over the size of
mapping table.


\section{Conclusion}

In this paper, we proposed Unified Storage Layer (USL) to solve two
problems: size of mapping table of SSD and compound garbage collection
of stacking log-structured systems. Mapping table in USL is managed
with disaggregate mapping which keeps two areas for different
purposes. Metadata area which is managed by page mapping is used for
metadata area of filesystem and main area for user data is stored in
no mapping zone of the storage. Instead of keeping a mapping table for
main area, disaggregate mapping directly maps LBAs of filesystem to
PBAs of storage device. The use of disaggregate mapping reduced the
overall metadata size of SSD to 1/54 compared to the de facto page
mapping scheme. Moreover, the static binding entirely eliminates the
root cause of compound garbage collection since the segment cleaning
of the log-structured filesystem directly consolidates the valid pages
in the Flash storage. WAF of USL is reduced by 40$\%$ and IOPS
increased about 77$\%$ against F2FS with a SSD. We believe that USL
not only minimizes the DRAM requirement for large scale SSDs but also
solves compound garbage collection in stacking log-structured system,
and it successfully increases the performance and life-time of the
storage.

\bibliographystyle{IEEEtran}
%\bibliographystyle{acm}
\bibliography{ref}

%\pagebreak
%\tableofcontents

\end{document}
